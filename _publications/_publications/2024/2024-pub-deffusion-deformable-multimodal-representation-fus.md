---
title:          "Deffusion: Deformable multimodal representation fusion for 3d semantic segmentation"
date:           2024-02-15 00:01:00 +0800
selected:       false
pub:            "Unknown Venue"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2024"

abstract: >-
  The complementarity between camera and LiDAR data makes fusion methods a promising approach to improve 3D semantic segmentation performance. Recent transformer-based methods have also demonstrated superiority in segmentation. However, multimodal solutions incorporating transformers are underexplored and face two key inherent difficulties: over-attention and noise from different modal data. To overcome these challenges, we propose a Deformable Multimodal Representation Fusion (DefFusion) framework consisting mainly of a Deformable Representation Fusion Transformer and Dynamic Representation Augmentation Modules. The Deformable Representation Fusion Transformer introduces the deformable mechanism in multimodal fusion, avoiding over-attention and improving efficiency by adaptively modeling a 2D key/value set for a given 3D query, thus enabling multimodal fusion with higher â€¦

cover:          assets/images/covers/default.png
authors:
  - Rongtao Xu
  - Changwei Wang
  - Duzhen Zhang
  - Man Zhang
  - Shibiao Xu
  - Weiliang Meng
  - Xiaopeng Zhang
links:
  Paper: https://ieeexplore.ieee.org/abstract/document/10610465/
---
