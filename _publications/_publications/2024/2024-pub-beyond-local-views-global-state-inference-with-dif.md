---
title:          "Beyond Local Views: Global State Inference with Diffusion Models for Cooperative Multi-Agent Reinforcement Learning"
date:           2024-02-15 00:01:00 +0800
selected:       false
pub:            "arXiv preprint arXiv:2408.09501"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2024"

abstract: >-
  In partially observable multi-agent systems, agents typically only have access to local observations. This severely hinders their ability to make precise decisions, particularly during decentralized execution. To alleviate this problem and inspired by image outpainting, we propose State Inference with Diffusion Models (SIDIFF), which uses diffusion models to reconstruct the original global state based solely on local observations. SIDIFF consists of a state generator and a state extractor, which allow agents to choose suitable actions by considering both the reconstructed global state and local observations. In addition, SIDIFF can be effortlessly incorporated into current multi-agent reinforcement learning algorithms to improve their performance. Finally, we evaluated SIDIFF on different experimental platforms, including Multi-Agent Battle City (MABC), a novel and flexible multi-agent reinforcement learning environment we developed. SIDIFF achieved desirable results and outperformed other popular algorithms.

cover:          assets/images/covers/default.png
authors:
  - Zhiwei Xu
  - Hangyu Mao
  - Nianmin Zhang
  - Xin Xin
  - Pengjie Ren
  - Dapeng Li
  - Bin Zhang
  - Guoliang Fan
  - Zhumin Chen
  - Changwei Wang
  - Jiangjin Yin
links:
  Paper: https://arxiv.org/abs/2408.09501
---
