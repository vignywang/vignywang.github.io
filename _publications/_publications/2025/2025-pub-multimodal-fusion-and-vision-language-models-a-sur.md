---
title:          "Multimodal fusion and vision-language models: A survey for robot vision"
date:           2025-02-15 00:01:00 +0800
selected:       false
pub:            "Unknown Venue"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  Robot vision has significantly benefited from advances in multimodal fusion techniques and vision-language models (VLMs). Starting from robot vision tasks, we have constructed a task-centric analytical framework and reviewed the application and development of multimodal fusion methods and VLMs in the field of robot vision. For semantic scene understanding tasks, we have categorized fusion methods into three types: encoder–decoder frameworks, attention-based architectures, and graph neural networks. Additionally, we analyze the architectural characteristics and practical implementations of these fusion strategies in key tasks such as simultaneous localization and mapping (SLAM), 3D object detection, navigation, and robotic manipulation. We further explore the evolutionary path of VLMs, from early designs centered on modal alignment to unified and context-aware architectures, increasingly enhancing …

cover:          assets/images/covers/default.png
authors:
  - Xiaofeng Han
  - Shunpeng Chen
  - Zenghuang Fu
  - Zhe Feng
  - Lue Fan
  - Dong An
  - Changwei Wang
  - Li Guo
  - Weiliang Meng
  - Xiaopeng Zhang
  - Rongtao Xu
  - Shibiao Xu
links:
  Paper: https://www.sciencedirect.com/science/article/pii/S1566253525007249
---
