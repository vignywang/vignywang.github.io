---
title:          "Collaboration Wins More: Dual-Modal Collaborative Attention Reinforcement for Mitigating Large Vision Language Models Hallucination"
date:           2025-02-15 00:01:00 +0800
selected:       false
pub:            "Unknown Venue"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in visual-language understanding for downstream multimodal tasks. However, these models often generate descriptions containing objects or details not present in the input image, a phenomenon commonly referred to as ''hallucination''. Existing methods focus solely on single-side hallucination mitigation: Intra-modal-only reinforcement (e.g. visual attention enhancement) ignores prompt-based guidance; Inter-modal-only correlation correction may introduce low-information visual tokens to mislead reasoning. To tackle this challenge, we propose Dual-Modal Collaborative Attention Reinforcement (DuCAR). Specifically, DuCAR is equipped with intra-visual CLS-driven sampling and cross-modal dynamic sampling, extracting important visual tokens guided by intra- and inter-modal joint information. During the multimodal fusion â€¦

cover:          assets/images/covers/default.png
authors:
  - Jiye Xie
  - Yifei Gao
  - Liangliang You
  - Xiang Xu
  - Haoran Xu
  - Zhiqiang Kou
  - Kexue Fu
  - Youyang Qu
  - Wenjie Yang
  - Jianwei Guo
  - Weiliang Meng
  - Longxiang Gao
  - Haoran Yang
  - Changwei Wang
  - Yu Zhang
links:
  Paper: https://dl.acm.org/doi/abs/10.1145/3746027.3755320
---
