---
title:          "PanoDit: Panoramic videos generation with diffusion transformer"
date:           2025-02-15 00:01:00 +0800
selected:       false
pub:            "Proceedings of the AAAI Conference on Artificial Intelligence"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  As immersive experiences become increasingly popular, panoramic video has garnered significant attention in both research and applications. The high cost associated with capturing panoramic video underscores the need for efficient prompt-based generation methods. Although recent text-to-video (T2V) diffusion techniques have shown potential in standard video generation, they face challenges when applied to panoramic videos due to substantial differences in content and motion patterns. In this paper, we propose PanoDiT, a framework that utilizes the Diffusion Transformer (DiT) architecture to generate panoramic videos from text descriptions. Unlike traditional methods that rely on UNet-based denoising, our method leverages a transformer architecture for denoising, incorporating both temporal and global attention mechanisms. This ensures coherent frame generation and smooth motion transitions, offering distinct advantages in long-horizon generation tasks. To further enhance motion and consistency in the generated videos, we introduce DTM-LoRA and two panoramic-specific losses. Compared to previous methods, our PanoDiT achieves state-of-the-art performance across various evaluation metrics and user study, with code is available in the supplementary material.

cover:          assets/images/covers/default.png
authors:
  - Muyang Zhang
  - Yuzhi Chen
  - Rongtao Xu
  - Changwei Wang
  - JinMing Yang
  - Weiliang Meng
  - Jianwei Guo
  - Huihuang Zhao
  - Xiaopeng Zhang
links:
  Paper: https://ojs.aaai.org/index.php/AAAI/article/view/33089
---
