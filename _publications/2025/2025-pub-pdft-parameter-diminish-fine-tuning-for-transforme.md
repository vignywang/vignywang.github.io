---
title:          "PDFT: parameter-diminish fine-tuning for transformer-based models: M. Zhang et al."
date:           2025-01-01 00:01:00 +0800
selected:       false
pub:            "The Visual Computer"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  Recent research in deep learning has focused on various large models that excel in many non-industrial environments. However, deploying these models in practical applications often presents significant computational and storage challenges. The research community commonly uses knowledge distillation methods to optimize the spatial dimensions of large models to meet industrial requirements. Nevertheless, knowledge distillation typically separates the processes of knowledge transfer and downstream adaptation. To solve this, we propose Parameter-Diminish Fine-Tuning (PDFT), a technique that compresses Transformer-based large models during fine-tuning, enabling initial lightweighting on downstream datasets without significantly sacrificing performance. We further introduce a Probabilistic Stepping Replacement (PSR) method and an advanced training schedule to enhance the performance of PDFT â€¦

cover:          assets/images/covers/default.png
authors:
  - Muyang Zhang
  - Weiliang Meng
  - Mingda Jia
  - Jiaming Gu
  - Yihua Shao
  - Changwei Wang
  - Rongtao Xu
  - Zhihao Ma
  - Xiaopeng Zhang
links:
  Paper: https://link.springer.com/article/10.1007/s00371-025-03973-y
---
