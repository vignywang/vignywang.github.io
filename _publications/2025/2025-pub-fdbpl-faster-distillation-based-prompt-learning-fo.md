---
title:          "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation"
date:           2025-01-01 00:01:00 +0800
selected:       false
pub:            "Expert Systems with Applications"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  Prompt learning as a parameter-efficient method that has been widely adopted to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt design requires domain expertise and iterative optimization, soft-prompt methods rely heavily on task-specific hard labels, limiting their generalization to unseen categories. Recent popular distillation-based prompt learning methods improve generalization by exploiting larger teacher VLMs and unsupervised knowledge transfer, yet their repetitive teacher model online inference sacrifices the inherent training efficiency advantage of prompt learning. In this paper, we propose Faster Distillation-Based Prompt Learning (FDBPL), which addresses these issues by sharing soft supervision contexts across multiple training stages and implementing accelerated I/O. Furthermore, FDBPL introduces a region-aware prompt learning paradigm with dual positive â€¦

cover:          assets/images/covers/default.png
authors:
  - Zherui Zhang
  - Jiaxin Wu
  - Changwei Wang
  - Rongtao Xu
  - Longzhao Huang
  - Wenhao Xu
  - Wenbo Xu
  - Li Guo
  - Shibiao Xu
links:
  Paper: https://www.sciencedirect.com/science/article/pii/S0957417425021967
---
