---
title:          "Representation convergence: Mutual distillation is secretly a form of regularization"
date:           2025-02-15 00:01:00 +0800
selected:       false
pub:            "arXiv preprint arXiv:2501.02481"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  In this paper, we argue that mutual distillation between reinforcement learning policies serves as an implicit regularization, preventing them from overfitting to irrelevant features. We highlight two separate contributions: (i) Theoretically, for the first time, we prove that enhancing the policy robustness to irrelevant features leads to improved generalization performance. (ii) Empirically, we demonstrate that mutual distillation between policies contributes to such robustness, enabling the spontaneous emergence of invariant representations over pixel inputs. Ultimately, we do not claim to achieve state-of-the-art performance but rather focus on uncovering the underlying principles of generalization and deepening our understanding of its mechanisms.

cover:          assets/images/covers/default.png
authors:
  - Zhengpeng Xie
  - Jiahang Cao
  - Changwei Wang
  - Fan Yang
  - Marco Hutter
  - Qiang Zhang
  - Jianxiong Zhang
  - Renjing Xu
links:
  Paper: https://arxiv.org/abs/2501.02481
---
