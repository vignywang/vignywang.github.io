---
title:          "Unifying Structure and Activation: A Comprehensive Approach of Parameter and Memory Efficient Transfer Learning"
date:           2025-01-01 00:01:00 +0800
selected:       false
pub:            "arXiv e-prints"
pub_last:       ' <span class="badge badge-pill badge-custom badge-dark">Scholar Paper</span>'
pub_date:       "2025"

abstract: >-
  Parameter-efficient transfer learning (PETL) aims to reduce the scales of pre-trained models for multiple downstream tasks. However, as the models keep scaling up, the memory footprint of existing PETL methods is not significantly reduced compared to the reduction of learnable parameters. This limitation hinders the practical deployment of PETL methods on memory-constrained devices. To this end, we proposed a new PETL framework, called Structure to Activation (S2A), to reduce the memory footprint of activation during fine-tuning. Specifically, our framework consists of: 1) Activation modules design (ie bias, prompt and side modules) in the parametric model structure, which results in a significant reduction of adjustable parameters and activation memory 2) 4-bit quantisation of activations based on their derivatives for non-parametric structures (eg, nonlinear functions), which maintains accuracy while â€¦

cover:          assets/images/covers/default.png
authors:
  - Tian Jin
  - Enjun Du
  - Changwei Wang
  - Wenhao Xu
  - Ding Luo
links:
  Paper: https://ui.adsabs.harvard.edu/abs/2025arXiv250308154J/abstract
---
